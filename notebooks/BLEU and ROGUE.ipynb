{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3c1f4a-9089-4882-97bb-7716f0d861c5",
   "metadata": {},
   "source": [
    "# BLEU Scores without Resume and JD references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ffa7bf-448d-40a8-8cc1-4db3615b3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0954\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from a file and split into tokens.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text.split()\n",
    "\n",
    "# Load the generated and control letters\n",
    "generated_text = load_text(\"GeneratedLetter.txt\")\n",
    "control_text = load_text(\"ExperimentControlLetter.txt\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "# Use smoothing function to handle potential zero scores from short texts\n",
    "smoothing = SmoothingFunction().method4\n",
    "bleu_score = sentence_bleu([control_text], generated_text, smoothing_function=smoothing)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecc100-3221-4811-ba64-7fba32bb747f",
   "metadata": {},
   "source": [
    "BLEU (Bilingual Evaluation Understudy) Score:\r\n",
    "BLEU score is a widely used metric for machine translation tasks, where the goal is to automatically translate text from one language to another. It was proposed as a way to assess the quality of machine-generated translations by comparing them to a set of reference translations provided by human translators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed648ab1-71f9-4711-bffe-1fa126dd9d64",
   "metadata": {},
   "source": [
    "# BLEU Scores with Resume and JD references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e186328-4a8b-4967-a162-9e011a787174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.3620\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from a file and split into tokens.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text.split()\n",
    "\n",
    "# Load the generated and control letters\n",
    "generated_text = load_text(\"GeneratedLetter.txt\")\n",
    "control_text = load_text(\"ExperimentWithReferences.txt\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "# Use smoothing function to handle potential zero scores from short texts\n",
    "smoothing = SmoothingFunction().method4\n",
    "bleu_score = sentence_bleu([control_text], generated_text, smoothing_function=smoothing)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8e777-28c5-4000-a2ef-11a75e358095",
   "metadata": {},
   "source": [
    "We can see that the BLEU Scores greatly improve when the generated content is compared to text which has been written with the Resume and JD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8efc3-3be7-4bb1-8eed-d723b1803e61",
   "metadata": {},
   "source": [
    "# ROGUE Scores without Resume and JD references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14769949-44f7-42bd-8d7a-2119d377fc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: Score(precision=0.5119760479041916, recall=0.5428571428571428, fmeasure=0.5269645608628659)\n",
      "ROUGE-2: Score(precision=0.18618618618618618, recall=0.19745222929936307, fmeasure=0.19165378670788258)\n",
      "ROUGE-L: Score(precision=0.23952095808383234, recall=0.25396825396825395, fmeasure=0.2465331278890601)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from a file as a single string.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load the generated and control letters\n",
    "generated_text = load_text(\"GeneratedLetter.txt\")\n",
    "control_text = load_text(\"ExperimentControlLetter.txt\")\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(control_text, generated_text)\n",
    "\n",
    "# Print out the ROUGE scores\n",
    "print(\"ROUGE-1:\", scores['rouge1'])\n",
    "print(\"ROUGE-2:\", scores['rouge2'])\n",
    "print(\"ROUGE-L:\", scores['rougeL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fdad73-f073-4d16-954a-054dc70b8cda",
   "metadata": {},
   "source": [
    "Precision: How much of the generated text overlaps with the reference. <br>\n",
    "Recall: How much of the reference is covered by the generated text. <br>\n",
    "F1 Score: A balance between precision and recall. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00813a96-68ef-41a1-b938-3d8844d9c41e",
   "metadata": {},
   "source": [
    "ROUGE-1: Measures the overlap of unigrams (single words) between the generated and reference texts. <br>\n",
    "ROUGE-2: Measures the overlap of bigrams (two-word pairs) between the generated and reference texts. <br>\n",
    "ROUGE-L: Measures the longest common subsequence (LCS) between the generated and reference texts. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb799d9-5ff9-4857-9241-64e4b709c0d6",
   "metadata": {},
   "source": [
    "ROUGE-1 is good for measuring basic word-level similarity. <br>\n",
    "ROUGE-2 gives insight into phrase-level similarity and fluency. <br>\n",
    "ROUGE-L focuses on capturing structural similarity, as it identifies the longest matching sequence in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedfd50-12aa-49fa-9dba-6de1f17cf769",
   "metadata": {},
   "source": [
    "# ROGUE Scores with Resume and JD references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010f3a61-ffd4-4953-9e4b-39a9520a9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: Score(precision=0.6706586826347305, recall=0.7066246056782335, fmeasure=0.6881720430107526)\n",
      "ROUGE-2: Score(precision=0.4444444444444444, recall=0.46835443037974683, fmeasure=0.4560862865947611)\n",
      "ROUGE-L: Score(precision=0.4251497005988024, recall=0.4479495268138801, fmeasure=0.4362519201228879)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from a file as a single string.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Load the generated and control letters\n",
    "generated_text = load_text(\"GeneratedLetter.txt\")\n",
    "control_text = load_text(\"ExperimentWithReferences.txt\")\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(control_text, generated_text)\n",
    "\n",
    "# Print out the ROUGE scores\n",
    "print(\"ROUGE-1:\", scores['rouge1'])\n",
    "print(\"ROUGE-2:\", scores['rouge2'])\n",
    "print(\"ROUGE-L:\", scores['rougeL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e5257-c66a-4cc8-a11d-4a1e51678788",
   "metadata": {},
   "source": [
    "We can see that the ROUGE Scores greatly improve when the generated content is compared to text which has been written with the Resume and JD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a29ba1-bdbe-4451-8c9c-3d3381a0de7e",
   "metadata": {},
   "source": [
    "# Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42803a2-4f94-4461-aac2-d3a16694990c",
   "metadata": {},
   "source": [
    "[Your Name] [Your Address] | [Your Email] | [Your Phone Number] | [Your LinkedIn Profile]\r\n",
    "\r\n",
    "[Date]\r\n",
    "\r\n",
    "[Hiring Manager Name] [Hiring Manager Title] Presto [Presto Address]\r\n",
    "\r\n",
    "Dear [Hiring Manager Name],\r\n",
    "\r\n",
    "I am writing to express my keen interest in the Generative AI Intern position at Presto, as advertised on [Platform where you saw the ad]. As a final year B.Tech. student specializing in Artificial Intelligence and Machine Learning at Vellore Institute of Technology, I am deeply passionate about leveraging AI to create innovative solutions that solve real-world problems.\r\n",
    "\r\n",
    "My academic background and practical experience have equipped me with a strong foundation in generative AI. I have a proven track record of developing and deploying cutting-edge AI models. In my internship at Pharmonix Biologicals Pvt. Ltd., I designed and developed a generative AI chatbot using the GPT 3.5-turbo LLM model, which significantly enhanced their B2B e-commerce website. Furthermore, my experience leading projects like the \"SMS Spam - Not Spam Prediction with Front End Application\" and \"Health monitoring using ESP32 microcontroller\" has honed my skills in Python, machine learning algorithms, and front-end development.\r\n",
    "\r\n",
    "I am particularly excited about the opportunity to contribute to Presto's mission of revolutionizing the restaurant industry through AI. My understanding of AI principles, coupled with my hands-on experience in prompt engineering and model fine-tuning, makes me confident that I can quickly learn and contribute to your team's efforts in developing innovative drive-thru voice automation solutions.\r\n",
    "\r\n",
    "I am eager to learn from experienced AI professionals at Presto and gain practical experience in developing and deploying generative AI models in a real-world setting. I am available for a full-time internship starting [Start Date].\r\n",
    "\r\n",
    "Thank you for your time and consideration. I look forward to hearing from you soon.\r\n",
    "\r\n",
    "Sincerely, Akshay Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eae6ad-ac80-46cc-8409-3ee8fa25f6ba",
   "metadata": {},
   "source": [
    "## Hallucinations have been fixed and limited to when specific details have to be added, like the [Platform where you saw the ad] <br>\n",
    "## The LLM is instructed to not add filler hallucinations, but at the same time will not create specific details like \"[Platform where you saw the ad]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a5614-6918-43a1-a303-99c223aec02a",
   "metadata": {},
   "source": [
    "## Hallucination test When no Resume Is Given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf5d42-25bc-4939-8599-0df58d2617f1",
   "metadata": {},
   "source": [
    "[Your Name] [Your Address] [Your Phone Number] [Your Email Address]\n",
    "\n",
    "[Date]\n",
    "\n",
    "[Hiring Manager Name (if known)] [Hiring Manager Title] Presto [Company Address]\n",
    "\n",
    "Dear [Hiring Manager Name],\n",
    "\n",
    "I am writing to express my enthusiastic interest in the Generative AI Intern position at Presto, as advertised on [Platform where you found the job posting]. With my [Your Degree] in [Your Major] from [Your University] and my passion for applying cutting-edge AI technologies to solve real-world problems, I am confident I can make a valuable contribution to your team.\n",
    "\n",
    "During my academic career, I have developed a strong foundation in machine learning principles and algorithms, gaining practical experience with programming languages like Python and frameworks such as TensorFlow and PyTorch. My research focused on [Mention a relevant project or skill related to generative AI or prompt engineering], which honed my abilities in [Specific skills related to the job description]. I am particularly excited by Presto's commitment to revolutionizing the restaurant industry with innovative AI solutions, and I believe my skills in prompt engineering and AI model development would be a valuable asset to your drive-thru voice automation efforts.\n",
    "\n",
    "I am eager to learn from experienced AI professionals at Presto and contribute to the development of AI-driven features for Presto Voice™. I am a highly motivated and collaborative individual with a strong work ethic and a commitment to continuous learning.\n",
    "\n",
    "Thank you for your time and consideration. I look forward to the opportunity to discuss my qualifications further and learn more about this exciting role.\n",
    "\n",
    "Sincerely,\n",
    "\n",
    "[Your Name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d7122-27b9-495d-bd57-98531612c8d6",
   "metadata": {},
   "source": [
    "## In this case, i have not given personal details using which the letter can be made. Still here there are only 2 hallucinations (Because of insufficient information)  <br>\n",
    "## It has been instructed NOT TO CREATE details by lying, instead the user will add it later which is more truthful\n",
    "## The two hallucinations are about \"Projects rerlated to Gen AI\" and \"Skills related to the JD\" which are very important and should not be inaccurate and hence is hallucinated to avoid misrepresentation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c7825-23f8-4586-82b0-2b020af4e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
